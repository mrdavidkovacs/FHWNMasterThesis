%!TEX root = ../main.tex

% \convertto{cm}{\the\textwidth}

% \convertto{cm}{\the\textheight}

\newcommand{\indicesCaption}[1]{Indices of #1 within the evaluation time frame}
\newcommand{\resultsCaption}[1]{Normalized sentiment and share values of #1 within the evaluation time frame}
\newcommand{\sentimentsCaption}[1]{Summarized and normalized sentiment values of #1 within the evaluation time frame}
\newcommand{\tweetsCaption}[1]{Collection time frames of tweets for #1}
\newcommand{\confusionCaption}[1]{Confusion matrix of classifier #1}
\newcommand{\hyperCaption}[1]{Determined hyper-parameters for pipeline of classifier #1}
\newcommand{\oppositeCaption}[1]{Days with opposite sentiment values for the company #1}

This chapter describes the performed analysis steps and discusses its results.
First, the final dataset is described in \cref{s:analysis-datasets}.
Second, the best hyper-parameters for each classifier are depicted in \cref{s:analysis-pipelines}.
Third, the sentiments time series per company are discussed in \cref{s:analysis-sentiments}.
Last but not least, the various time series are compared to the corresponding share price time series in \cref{s:analysis-granger}.

\section{Datasets}
\label{s:analysis-datasets}
% dataset description
%  - per company
%  - time series

This section describes the captured data sets in detail.
It will include both the captured indices and also captured tweets.
Furthermore, the description is divided by the owning company.

\subsection{\ford}
\label{ss:analysis-datasets-ford}

% From         To Days  Tweets
% DG#0  2018-02-28 2018-04-13   45 1536477
% DG#3  2018-06-26 2018-08-01   37 1544756
% DG#4  2018-08-20 2018-09-05   17  664214
% Total 2018-02-28 2018-09-05   99 3745447

For the \ford\ \num{3745447} english tweets have been captured in total between \printdate{2018-02-28} and \printdate{2018-09-05}.
But as stated earlier in \cref{ss:casestudy-gatherdata-tweets} this was not a continuous time frame.
The various collected time frames, number of consecutive days in each frame and collected tweets in that frame are depicted in \cref{tab:anaylsis-datasets-ford}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^l ^l ^r ^r}
      \hline
      \rowstyle{\bfseries}
                & From & To & \# Days & \# Tweets \\ \hline
        Group 1 & \printdate{2018-02-28} & \printdate{2018-04-13} &   \num{45} & \num{1536477} \\
        Group 2 & \printdate{2018-06-26} & \printdate{2018-08-01} &   \num{37} & \num{1544756} \\
        Group 3 & \printdate{2018-08-20} & \printdate{2018-09-05} &   \num{17} & \num{664214} \\ \hline
        Total   & \printdate{2018-02-28} & \printdate{2018-09-05} &   \num{99} & \num{3745447} \\ \hline
    \end{tabular}
  
    \caption{\tweetsCaption{\ford}}
    \label{tab:anaylsis-datasets-ford}
\end{table}

Therefore, share prices have been downloaded for the entire given time frame which is depicted in \cref{fig:analysis-indices-ford}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/indices-ford.tex}
    \caption{\indicesCaption{\ford}}
    \label{fig:analysis-indices-ford}
\end{figure}    

\subsection{\gm}
\label{ss:analysis-datasets-gm}

% From         To Days Tweets
% DG#0  2018-06-26 2018-08-01   37 270276
% DG#1  2018-08-20 2018-09-06   18 143541
% Total 2018-06-26 2018-09-06   55 413817

For the \gm\ \num{413817} english tweets have been captured in total between \printdate{2018-06-26} and \printdate{2018-09-06}.
But as stated earlier in \cref{ss:casestudy-gatherdata-tweets} this was not a continuous time frame.
The various collected time frames, number of consecutive days in each frame and collected tweets in that frame are depicted in \cref{tab:anaylsis-datasets-gm}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^l ^l ^r ^r}
      \hline
      \rowstyle{\bfseries}
                & From & To & \# Days & \# Tweets \\ \hline
        Group 1 & \printdate{2018-06-26} & \printdate{2018-08-01} &   \num{37} & \num{270276} \\
        Group 2 & \printdate{2018-08-20} & \printdate{2018-09-06} &   \num{18} & \num{143541} \\ \hline
        Total   & \printdate{2018-06-26} & \printdate{2018-09-06} &   \num{99} & \num{413817} \\ \hline
    \end{tabular}
  
    \caption{\tweetsCaption{\gm}}
    \label{tab:anaylsis-datasets-gm}
\end{table}

Therefore, share prices have been downloaded for the entire given time frame which is depicted in \cref{fig:analysis-indices-gm}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/indices-gm.tex}
    \caption{\indicesCaption{\gm}}
    \label{fig:analysis-indices-gm}
\end{figure}   

\subsection{\hyundai}
\label{ss:analysis-datasets-hyundai}

% From         To Days Tweets
% DG#0  2018-02-28 2018-04-08   40 363840
% DG#4  2018-06-26 2018-08-01   37 232092
% DG#5  2018-08-20 2018-09-07   19 101289
% Total 2018-02-28 2018-09-07   96 697221

For the \hyundai\ \num{697221} english tweets have been captured in total between \printdate{2018-02-28} and \printdate{2018-09-07}.
But as stated earlier in \cref{ss:casestudy-gatherdata-tweets} this was not a continuous time frame.
The various collected time frames, number of consecutive days in each frame and collected tweets in that frame are depicted in \cref{tab:anaylsis-datasets-hyundai}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^l ^l ^r ^r}
      \hline
      \rowstyle{\bfseries}
                & From & To & \# Days & \# Tweets \\ \hline
        Group 1 & \printdate{2018-02-28} & \printdate{2018-04-08} &   \num{40} & \num{363840} \\
        Group 2 & \printdate{2018-06-26} & \printdate{2018-08-01} &   \num{37} & \num{232092} \\
        Group 3 & \printdate{2018-08-20} & \printdate{2018-09-07} &   \num{19} & \num{101289} \\ \hline
        Total   & \printdate{2018-02-28} & \printdate{2018-09-07} &   \num{96} & \num{697221} \\ \hline
    \end{tabular}
  
    \caption{\tweetsCaption{\hyundai}}
    \label{tab:anaylsis-datasets-hyundai}
\end{table}

Therefore, share prices have been downloaded for the entire given time frame which is depicted in \cref{fig:analysis-indices-hyundai}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/indices-hyundai.tex}
    \caption{\indicesCaption{\hyundai}}
    \label{fig:analysis-indices-hyundai}
\end{figure}   

\subsection{\toyota}
\label{ss:analysis-datasets-toyota}

% From         To Days Tweets
% DG#0  2018-06-26 2018-08-01   37 331715
% DG#1  2018-08-20 2018-09-07   19 157198
% Total 2018-06-26 2018-09-07   56 488913

For the \toyota\ \num{488913} english tweets have been captured in total between \printdate{2018-06-26} and \printdate{2018-09-07}.
But as stated earlier in \cref{ss:casestudy-gatherdata-tweets} this was not a continuous time frame.
The various collected time frames, number of consecutive days in each frame and collected tweets in that frame are depicted in \cref{tab:anaylsis-datasets-toyota}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^l ^l ^r ^r}
      \hline
      \rowstyle{\bfseries}
                & From & To & \# Days & \# Tweets \\ \hline
        Group 1 & \printdate{2018-06-26} & \printdate{2018-08-01} &   \num{37} & \num{331715} \\
        Group 2 & \printdate{2018-08-20} & \printdate{2018-09-07} &   \num{19} & \num{157198} \\ \hline
        Total   & \printdate{2018-02-28} & \printdate{2018-09-07} &   \num{56} & \num{488913} \\ \hline
    \end{tabular}
  
    \caption{\tweetsCaption{\toyota}}
    \label{tab:anaylsis-datasets-toyota}
\end{table}

Therefore, share prices have been downloaded for the given time frame which is depicted in \cref{fig:analysis-indices-toyota}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/indices-toyota.tex}
    \caption{\indicesCaption{\toyota}}
    \label{fig:analysis-indices-toyota}
\end{figure}   

\subsection{\vw}
\label{ss:analysis-datasets-vw}

%             From         To Days  Tweets
% DG#0  2018-02-28 2018-04-08   40 3001535
% DG#4  2018-06-26 2018-07-03    8  765042
% DG#5  2018-07-19 2018-08-01   14  955986
% DG#6  2018-08-20 2018-09-05   17 1496787
% Total 2018-02-28 2018-09-05   79 6219350

For the \vw\ \num{6219350} english tweets have been captured in total between \printdate{2018-02-28} and \printdate{2018-09-05}.
But as stated earlier in \cref{ss:casestudy-gatherdata-tweets} this was not a continuous time frame.
The various collected time frames, number of consecutive days in each frame and collected tweets in that frame are depicted in \cref{tab:anaylsis-datasets-vw}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^l ^l ^r ^r}
      \hline
      \rowstyle{\bfseries}
                & From & To & \# Days & \# Tweets \\ \hline
        Group 1 & \printdate{2018-02-28} & \printdate{2018-04-08} &   \num{40} & \num{3001535} \\
        Group 2 & \printdate{2018-06-26} & \printdate{2018-07-03} &   \num{08} & \num{765042} \\
        Group 3 & \printdate{2018-07-19} & \printdate{2018-08-01} &   \num{14} & \num{955986} \\
        Group 4 & \printdate{2018-08-20} & \printdate{2018-09-05} &   \num{17} & \num{1496787} \\ \hline
        Total   & \printdate{2018-02-28} & \printdate{2018-09-05} &   \num{96} & \num{6219350} \\ \hline
    \end{tabular}
  
    \caption{\tweetsCaption{\vw}}
    \label{tab:anaylsis-datasets-vw}
\end{table}

Therefore, share prices have been downloaded for the given time frame which is depicted in \cref{fig:analysis-indices-vw}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/indices-vw.tex}
    \caption{\indicesCaption{\vw}}
    \label{fig:analysis-indices-vw}
\end{figure}   

\section{Hyper-parameters}
\label{s:analysis-pipelines}
% best estimator pipeline

This section presents the results of the model selector GridSearchCV as described in \cref{s:casestudy-sentiment}.
GridSearchCV has been trained and tested using the Twitter corpus of \ac{NLTK} (\emph{nltk.corpus.twitter\_samples}) which contains each \num{5000} positive and \num{5000} negative tweets.

\subsection{TextBlob Classifier}
\label{ss:analysis-pipeline-textblob}

As previously stated \emph{TextBlob} is a pre-trained classifier which works out-of-the-box.
It can be seen as kind of a base-line.

As the TextBlob classifies text on a continuous scale between \texttt{-1} and \texttt{1} it also includes the possibility of a neutral sentiment: \texttt{0}.
In order to compare this classifier with the other tested ones we have to generalize the results of TextBlob using the following cases in \cref{eq:analysis-pipeline-textblob}.

\begin{equation}
Sentiment_{generalized} = 
    \begin{cases}
    1,  & Sentiment_{TextBlob} \geq 0 \\
    -1  & \text{otherwise}
    \end{cases}
    \label{eq:analysis-pipeline-textblob}
\end{equation}

These adjustments lead finally to an accuracy of \SI{97.34}{\percent} and an F-Measure of \num{97.37}.
The confusion matrix is depicted in \cref{tab:anaylsis-pipeline-textblob-confusion}.

% Accuracy: 97.34%
% F1 Score: 97.37
% Confusion Matrix:
%  [[4814 (TN)  186 (FP)]
%  [  80 (FN) 4920 (TN)]]

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r}
      \hline
        & \multicolumn{2}{c}{\bfseries Correct labels} \\
        \rowstyle{\bfseries}
        Predicted labels & Positive & Negative \\ \hline
        Positive & \num{4920}    & \num{186}  \\
        Negative & \num{80}      & \num{4814} \\ \hline
    \end{tabular}
  
    \caption{\confusionCaption{TextBlob}}
    \label{tab:anaylsis-pipeline-textblob-confusion}
\end{table}


\subsection{Naive Bayes Classifier}
\label{ss:analysis-pipeline-naivebayes}

The GridSearchCV model selector discovered \num{96} different possibilities for the \nb\ classifier using the defined hyper-parameters in \cref{s:casestudy-sentiment}.
As the classifier had to be trained the sample tweets have been split into training (\SI{80}{\percent}) and test (\SI{20}{\percent}) tweets.
Furthermore, the model selection process also performed five different folds which yields to five different training and test tweet sets.
Therefore, the confusion matrix is built upon \num{2000} test tweets.

GridSearchCV determined the best performing hyper-parameters which are depicted in \cref{tab:analysis-pipeline-naivebayes-hyperparameters}.
This best performing pipeline resulted in an accuracy of \SI{75.5}{\percent} and an F-Measure of \num{75.28}.
The confusion matrix is depicted in \cref{tab:anaylsis-pipeline-naivebayes-confusion}.

\begin{table}[!hbt]
    \centering
    \begin{tabular}{!l ^l ^l}
        \hline
        \rowstyle{\bfseries}
        Pipeline part & Variable & Tried values \\ \hline
        CountVectorizer & NGrams & 4-grams \\
        CountVectorizer & Stopwords & none \\
        CountVectorizer & Binary & true \\ \hline
        TF-IDF & Use IDF & false \\
        TF-IDF & Smooth IDF & true \\
        TF-IDF & Normalization & 'l1' \\ \hline
        Classifier & Alpha & $10^{-2}$ \\ \hline
    \end{tabular}

    \caption{\hyperCaption{Naive Bayes}}
    \label{tab:analysis-pipeline-naivebayes-hyperparameters}
\end{table}

% Fitting 5 folds for each of 96 candidates, totalling 480 fits
% 2019-02-22 13:10:02.230678 > training the classifier finished in 62.958709478378296
% {'memory': None,
%  'steps': [('vect',
%             CountVectorizer(analyzer='word', binary=True, decode_error='strict',
%         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
%         lowercase=True, max_df=1.0, max_features=None, min_df=1,
%         ngram_range=(1, 4), preprocessor=None, stop_words=None,
%         strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
%         tokenizer=None, vocabulary=None)),
%            ('tfidf',
%             TfidfTransformer(norm='l1', smooth_idf=True, sublinear_tf=False,
%          use_idf=False)),
%            ('clf',
%             MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))]}
% Best Score: 0.74700
% Accuracy: 75.50%
% F1 Score: 75.28
% Confusion Matrix:
%  [[764 248]
%  [242 746]]

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r}
      \hline
        & \multicolumn{2}{c}{\bfseries Correct labels} \\
        \rowstyle{\bfseries}
        Predicted labels & Positive & Negative \\ \hline
        Positive & \num{746}    & \num{248}  \\
        Negative & \num{242}    & \num{764} \\ \hline
    \end{tabular}
  
    \caption{\confusionCaption{Naive Bayes}}
    \label{tab:anaylsis-pipeline-naivebayes-confusion}
\end{table}

\subsection{Maximum Entropy Classifier}
\label{ss:analysis-pipeline-maximumentropy}

A similar result has been achieved using the \me\ classifier pipeline:
GridSearchCV discovered \num{192} different possibilities for the hyper-parameters.
The sample tweets have also been split into training and test tweets in a ratio of \num{80}:\num{20}.

The hyper-parameters of the best performing pipeline are depicted in \cref{tab:analysis-pipeline-maximumentropy-hyperparameters}.
These parameters yielded to a accuracy of \SI{76.15}{\percent} and a F-Measure of \num{75.45} and the confusion matrix is shown in \cref{tab:anaylsis-pipeline-maximumentropy-confusion}.

\begin{table}[!hbt]
    \centering
    \begin{tabular}{!l ^l ^l}
        \hline
        \rowstyle{\bfseries}
        Pipeline part & Variable & Tried values \\ \hline
        CountVectorizer & NGrams & 2-grams \\
        CountVectorizer & Stopwords & none \\
        CountVectorizer & Binary & false \\ \hline
        TF-IDF & Use IDF & false \\
        TF-IDF & Smooth IDF & true \\
        TF-IDF & Normalization & 'l1' \\ \hline
        Classifier & Solver & 'sag' \\
        Classifier & Multiclass & 'auto' \\ \hline
    \end{tabular}

    \caption{\hyperCaption{Maximum Entropy}}
    \label{tab:analysis-pipeline-maximumentropy-hyperparameters}
\end{table}

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r}
      \hline
        & \multicolumn{2}{c}{\bfseries Correct labels} \\
        \rowstyle{\bfseries}
        Predicted labels & Positive & Negative \\ \hline
        Positive & \num{733}    & \num{222}  \\
        Negative & \num{255}    & \num{790} \\ \hline
    \end{tabular}
  
    \caption{\confusionCaption{Maximum Entropy}}
    \label{tab:anaylsis-pipeline-maximumentropy-confusion}
\end{table}

% Fitting 5 folds for each of 192 candidates, totalling 960 fits
% 2019-02-22 13:23:02.324786 > training the classifier finished in 189.4081039428711
% {'memory': None,
%  'steps': [('vect',
%             CountVectorizer(analyzer='word', binary=False, decode_error='strict',
%         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
%         lowercase=True, max_df=1.0, max_features=None, min_df=1,
%         ngram_range=(1, 2), preprocessor=None, stop_words=None,
%         strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
%         tokenizer=None, vocabulary=None)),
%            ('tfidf',
%             TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=False,
%          use_idf=False)),
%            ('clf',
%             LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
%           intercept_scaling=1, max_iter=100, multi_class='auto',
%           n_jobs=None, penalty='l2', random_state=42, solver='sag',
%           tol=0.0001, verbose=0, warm_start=False))]}
% Best Score: 0.76150
% Accuracy: 76.15%
% F1 Score: 75.45
% Confusion Matrix:
%  [[790 222]
%  [255 733]]


\subsection{Support Vector Machine Classifier}
\label{ss:analysis-pipeline-supportvectormachine}

Last but not least the same model selection process has been performed for the \svm\ pipeline.
GridSearchCV discovered \num{480} different parameter possibilities in total, taking the five different folds into account.

The best pipeline resulted in an accuracy of \SI{75.5}{\percent} and a F-Measure of \num{75.81} using the hyper-parameters shown in \cref{tab:analysis-pipeline-supportvectormachine-hyperparameters}.
Furthermore, the confusion matrix of the \svm\ classifier pipeline is depicted in \cref{tab:anaylsis-pipeline-supportvectormachine-confusion}.

\begin{table}[!hbt]
    \centering
    \begin{tabular}{!l ^l ^l}
        \hline
        \rowstyle{\bfseries}
        Pipeline part & Variable & Tried values \\ \hline
        CountVectorizer & NGrams & 4-grams \\
        CountVectorizer & Stopwords & none \\
        CountVectorizer & Binary & true \\ \hline
        TF-IDF & Use IDF & true \\
        TF-IDF & Smooth IDF & true \\
        TF-IDF & Normalization & 'l2' \\ \hline
        Classifier & Tolerance & $10^{-3}$ \\
        Classifier & Loss & 'hinge' \\ \hline
    \end{tabular}

    \caption{\hyperCaption{Support Vector Machine}}
    \label{tab:analysis-pipeline-supportvectormachine-hyperparameters}
\end{table}

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r}
      \hline
        & \multicolumn{2}{c}{\bfseries Correct labels} \\
        \rowstyle{\bfseries}
        Predicted labels & Positive & Negative \\ \hline
        Positive & \num{768}    & \num{270}  \\
        Negative & \num{220}    & \num{742} \\ \hline
    \end{tabular}
  
    \caption{\confusionCaption{Support Vector Machine}}
    \label{tab:anaylsis-pipeline-supportvectormachine-confusion}
\end{table}


% Fitting 5 folds for each of 96 candidates, totalling 480 fits
% 2019-02-22 13:31:21.274959 > training the classifier finished in 65.60502362251282
% {'memory': None,
%  'steps': [('vect',
%             CountVectorizer(analyzer='word', binary=True, decode_error='strict',
%         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
%         lowercase=True, max_df=1.0, max_features=None, min_df=1,
%         ngram_range=(1, 4), preprocessor=None, stop_words=None,
%         strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
%         tokenizer=None, vocabulary=None)),
%            ('tfidf',
%             TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),
%            ('clf',
%             SGDClassifier(alpha=0.0001, average=False, class_weight=None,
%        early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
%        l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=2000,
%        n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',
%        power_t=0.5, random_state=42, shuffle=True, tol=0.001,
%        validation_fraction=0.1, verbose=0, warm_start=False))]}
% Best Score: 0.75875
% Accuracy: 75.50%
% F1 Score: 75.81
% Confusion Matrix:
%  [[742 270]
%  [220 768]]

\section{Sentiment Time Series}
\label{s:analysis-sentiments}
% depict time series

This section describes the sentiment time series which has been extracted from the collected tweets.
As each tweet data set has been analyzed using various sentiment detection classifiers the results are shown and described in detail in the corresponding subsections.

The sentiments time series has been normalized using \cref{eq:analysis-sentiments-normalization} to make them comparable which means that every specific point has been modified to fit into a normalized graph.
After this step each time series is scaled into a standard normal distribution with $\mu_z = 0$ and $\sigma_z = 1$.
Therefore all time series can be shown in a single shared graph.

\begin{equation}
    z_y = \frac{y - \mu_y}{\sigma_y}
    \label{eq:analysis-sentiments-normalization}
\end{equation}

\subsection{\ford}
\label{ss:analysis-sentiments-ford}

The various sentiment time series for \ford\ are depicted in \cref{fig:analysis-sentiments-ford} which are split upon three different groups as there were several days in between where no tweets have been captured.
But there were several days where the various sentiment classifiers led to opposite sentiments for the specific day.
These are listed in \cref{tab:analysis-sentiments-ford-opposite}.
For each date the number of analyzed tweets, the ration of retweets and the difference between minimum and maximum sentiment value is stated.
The difference are by means of standard deviation and only those days were taken into account which deviation between minimum and maximum values are greater than \texttt{1}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/sentiments-ford.tex}
    \caption{\sentimentsCaption{\ford}}
    \label{fig:analysis-sentiments-ford}
\end{figure} 

% 2018-07-13  3.51 -0.767  3.85 -0.435       82337    59897 0.576     0.716         3    37
% 13.07.2018	3,514015828	-0,766561511	3,848853484	-0,435083389	82337	0,575594719	4,615414995	-0,766561511	3,848853484

The most remarkable entry in \cref{tab:analysis-sentiments-ford-opposite} is of date \printdate{2018-07-13}.
As we can see in \cref{fig:analysis-sentiments-ford} on that specific day two of four sentiment classifiers rated the tweets as extremely positive (\num{3.51} for $S_{TB}$ and \num{3.85} for $S_{ME}$) whereas the other two classifiers rated the tweets of the day slightly negative (\num{-0.77} for $S_{NB}$ and \num{-0.44} for $S_{SVM}$ respectively).
There were \num{82337} tweets on that day which resulted in \num{26925} unique tweets as \SI{72.75}{\percent} were retweets.

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r ^r}
      \hline
        \rowstyle{\bfseries}
        Date & \# of tweets & RT ratio & Difference \\ \hline
        \printdate{2018-04-01}   &  \num{34629}   &  \SI{61.79}{\percent}   & \num{1.750} \\
        \printdate{2018-06-29}   &  \num{44416}   &  \SI{56.50}{\percent}   & \num{1.300} \\
        \printdate{2018-06-30}   &  \num{42860}   &  \SI{63.25}{\percent}   & \num{1.264} \\
        \printdate{2018-07-01}   &  \num{34419}   &  \SI{58.79}{\percent}   & \num{1.359} \\
        \printdate{2018-07-13}   &  \num{82337}   &  \SI{72.75}{\percent}   & \num{4.615} \\
        \printdate{2018-07-19}   &  \num{42224}   &  \SI{57.44}{\percent}   & \num{1.064} \\
        \printdate{2018-07-25}   &  \num{41487}   &  \SI{58.78}{\percent}   & \num{1.718} \\
        \printdate{2018-07-31}   &  \num{61599}   &  \SI{66.06}{\percent}   & \num{2.420} \\
        \printdate{2018-08-01}   &  \num{36016}   &  \SI{67.25}{\percent}   & \num{1.287} \\
        \printdate{2018-08-25}   &  \num{41092}   &  \SI{61.24}{\percent}   & \num{1.901} \\
        \printdate{2018-08-28}   &  \num{45198}   &  \SI{63.38}{\percent}   & \num{1.219} \\
        \printdate{2018-09-03}   &  \num{38024}   &  \SI{64.06}{\percent}   & \num{1.447} \\
        \printdate{2018-09-04}   &  \num{38075}   &  \SI{58.86}{\percent}   & \num{1.532} \\
        \hline        
    \end{tabular}
  
    \caption{\oppositeCaption{\ford}}
    \label{tab:analysis-sentiments-ford-opposite}
\end{table}

% Furthermore, we can see that 13 dates have at least one value above \num{2.326348} or below \num{-2.326348} which marks the point of \SI{99}{\percent} certainty.

\subsection{\gm}
\label{ss:analysis-sentiments-gm}

The various sentiment time series for \gm\ are depicted in \cref{fig:analysis-sentiments-gm} which are split upon two different groups.
There were two specific days which have a maximum normalized value above \texttt{3}: \printdate{2018-07-20} (\num{6.31}) and \printdate{2018-07-21} (\num{3.02}).
On both days were several thousand tweets where the keyword \emph{lincoln} matched an actors name: \emph{Andrew Lincoln}.

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/sentiments-gm.tex}
    \caption{\sentimentsCaption{\gm}}
    \label{fig:analysis-sentiments-gm}
\end{figure}

% date       SA_TB  SA_NB SA_ME SA_SVM tweet.count rt.count  Close Adj.Close dategroup count
% 2018-08-20  1.17 -0.993  2.00   1.08       17853    14224 -0.860    -0.873         1    18

For \gm\ there were only one day where several sentiment classifiers led to different sentiment classifications which can be seen in \cref{tab:analysis-sentiments-gm-opposite}.
On \printdate{2018-08-20} there were \num{17853} tweets captured whereas \num{14224} of these tweets were retweets.
On that day only the \nb\ sentiment classifier predicted a negative sentiment classification (\num{-0.99}) and all other classifiers had quite the same prediction (values between \num{1.08} and \num{2.00}).

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r ^r}
        \hline
        \rowstyle{\bfseries}
        Date & \# of tweets & RT ratio & Difference \\ \hline
        \printdate{2018-08-20}   &  \num{17853}   &  \SI{79.67}{\percent}   & \num{2.991} \\
        \hline        
      \end{tabular}
  
    \caption{\oppositeCaption{\gm}}
    \label{tab:analysis-sentiments-gm-opposite}
\end{table}

\subsection{\hyundai}
\label{ss:analysis-sentiments-hyundai}

The various sentiment time series for \hyundai\ are depicted in \cref{fig:analysis-sentiments-hyundai} which are split upon three different groups.
There were three days with a normalized sentiment value above \texttt{3}:

% date       SA_TB SA_NB SA_ME SA_SVM tweet.count rt.count Close Adj.Close dategroup count
% <date>     <dbl> <dbl> <dbl>  <dbl>       <int>    <int> <dbl>     <dbl>     <dbl> <int>
% 1 2018-03-21  4.92 -5.36  5.00 -5.00        30064    25793  1.26      1.27         0    40
% 2 2018-04-04  5.50  5.37  5.35 -0.607       35173    30212  1.41      1.42         0    40
% 3 2018-04-05  4.35  5.29  4.73  6.50        26285    22152  1.23      1.23         0    40

\begin{enumerate}
    \item 
        \printdate{2018-03-21} is a special day as it also includes the biggest difference of sentiments.
        The minimum sentiment value is \num{-5.36} for \nb classifier whereas the maximum is \num{5.00} for \me\ classifier.
        The other two values were also extreme: \num{4.92} and \num{-5.00} for TextBlob and \svm\ respectively.

    \item
        On \printdate{2018-04-04} were three sentiment classifiers above the threshold.
        The values for TextBlob, \nb and \me\ are \num{5.50}, \num{5.37} and \num{5.35} respectively.
        Only the value for \svm\ is \num{-0.61}.

    \item
        On \printdate{2018-04-05} all four sentiment classifiers were above the threshold.
        The values are \num{4.35}, \num{5.29}, \num{4.73} and \num{6.50} for $S_{TB}$, $S_{NB}$, $S_{ME}$ and $S_{SVM}$.
\end{enumerate}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/sentiments-hyundai.tex}
    \caption{\sentimentsCaption{\hyundai}}
    \label{fig:analysis-sentiments-hyundai}
\end{figure} 

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r ^r}
        \hline
        \rowstyle{\bfseries}
        Date & \# of tweets & RT ratio & Difference \\ \hline
        \printdate{2018-03-08}   &  \num{12531}   &  \SI{51.03}{\percent}   & \num{ 1.344} \\
        \printdate{2018-03-19}   &  \num{10462}   &  \SI{56.00}{\percent}   & \num{ 1.442} \\
        \printdate{2018-03-21}   &  \num{30064}   &  \SI{85.79}{\percent}   & \num{10.357} \\
        \printdate{2018-03-22}   &  \num{11023}   &  \SI{65.24}{\percent}   & \num{ 1.174} \\
        \printdate{2018-03-23}   &  \num{16702}   &  \SI{77.42}{\percent}   & \num{ 2.135} \\
        \printdate{2018-04-04}   &  \num{35173}   &  \SI{85.90}{\percent}   & \num{ 6.109} \\
        \printdate{2018-07-03}   &  \num{ 6112}   &  \SI{52.42}{\percent}   & \num{ 1.207} \\
        \printdate{2018-07-13}   &  \num{ 8447}   &  \SI{53.84}{\percent}   & \num{ 1.405} \\
        \printdate{2018-07-25}   &  \num{11072}   &  \SI{60.62}{\percent}   & \num{ 2.799} \\
        \printdate{2018-07-26}   &  \num{ 8898}   &  \SI{60.94}{\percent}   & \num{ 1.108} \\
        \hline        
      \end{tabular}
  
    \caption{\oppositeCaption{\hyundai}}
    \label{tab:analysis-sentiments-hyundai-opposite}
\end{table}

\subsection{\toyota}
\label{ss:analysis-sentiments-toyota}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/sentiments-toyota.tex}
    \caption{\sentimentsCaption{\toyota}}
    \label{fig:analysis-sentiments-toyota}
\end{figure} 

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r ^r}
        \hline
          \rowstyle{\bfseries}
          Date & \# of tweets & RT ratio & Difference \\ \hline
          \printdate{2018-07-11}   &  \num{12954}   &  \SI{59.43}{\percent}   & \num{2.215} \\
          \printdate{2018-07-12}   &  \num{12308}   &  \SI{59.46}{\percent}   & \num{1.785} \\
          \printdate{2018-07-19}   &  \num{12676}   &  \SI{59.92}{\percent}   & \num{2.011} \\
          \printdate{2018-07-20}   &  \num{25378}   &  \SI{79.71}{\percent}   & \num{9.598} \\
          \printdate{2018-07-21}   &  \num{20888}   &  \SI{74.17}{\percent}   & \num{6.277} \\
          \printdate{2018-07-22}   &  \num{18341}   &  \SI{79.73}{\percent}   & \num{5.595} \\
          \printdate{2018-07-23}   &  \num{12379}   &  \SI{58.83}{\percent}   & \num{1.928} \\
          \printdate{2018-08-21}   &  \num{ 6375}   &  \SI{36.24}{\percent}   & \num{1.006} \\
          \printdate{2018-09-07}   &  \num{ 3506}   &  \SI{39.79}{\percent}   & \num{1.218} \\  
          \hline        
      \end{tabular}
  
    \caption{\oppositeCaption{\toyota}}
    \label{tab:analysis-sentiments-toyota-opposite}
\end{table}

\subsection{\vw}
\label{ss:analysis-sentiments-vw}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/sentiments-vw.tex}
    \caption{\sentimentsCaption{\vw}}
    \label{fig:analysis-sentiments-vw}
\end{figure} 

\begin{table}[hbt]
    \centering
    \begin{tabular}{!>{\bfseries}l ^r ^r ^r}
        \hline
          \rowstyle{\bfseries}
          Date & \# of tweets & RT ratio & Difference \\ \hline
          \printdate{2018-03-07}   &  \num{ 76296}   &  \SI{54.06}{\percent}   & \num{1.131} \\
          \printdate{2018-03-11}   &  \num{115519}   &  \SI{77.27}{\percent}   & \num{2.134} \\
          \printdate{2018-03-12}   &  \num{111406}   &  \SI{73.04}{\percent}   & \num{4.335} \\
          \printdate{2018-03-18}   &  \num{ 54541}   &  \SI{54.40}{\percent}   & \num{1.049} \\
          \printdate{2018-03-26}   &  \num{ 62269}   &  \SI{54.56}{\percent}   & \num{1.120} \\
          \printdate{2018-03-27}   &  \num{ 63639}   &  \SI{53.20}{\percent}   & \num{1.224} \\
          \printdate{2018-03-29}   &  \num{ 79754}   &  \SI{60.36}{\percent}   & \num{1.175} \\
          \printdate{2018-03-30}   &  \num{112736}   &  \SI{47.87}{\percent}   & \num{3.264} \\
          \printdate{2018-04-02}   &  \num{ 94728}   &  \SI{71.79}{\percent}   & \num{1.381} \\
          \printdate{2018-04-03}   &  \num{113563}   &  \SI{74.31}{\percent}   & \num{1.701} \\
          \printdate{2018-04-05}   &  \num{ 79707}   &  \SI{60.36}{\percent}   & \num{1.149} \\
          \printdate{2018-04-06}   &  \num{ 90645}   &  \SI{68.59}{\percent}   & \num{4.686} \\
          \printdate{2018-04-07}   &  \num{ 19291}   &  \SI{71.32}{\percent}   & \num{3.284} \\
          \printdate{2018-04-08}   &  \num{   459}   &  \SI{60.35}{\percent}   & \num{2.601} \\
          \printdate{2018-06-27}   &  \num{155361}   &  \SI{78.94}{\percent}   & \num{2.750} \\
          \printdate{2018-06-28}   &  \num{172329}   &  \SI{80.39}{\percent}   & \num{6.202} \\
          \printdate{2018-07-19}   &  \num{ 19640}   &  \SI{58.33}{\percent}   & \num{2.124} \\
          \printdate{2018-07-21}   &  \num{105529}   &  \SI{73.38}{\percent}   & \num{3.078} \\
          \printdate{2018-07-23}   &  \num{ 83453}   &  \SI{68.90}{\percent}   & \num{2.281} \\
          \printdate{2018-07-27}   &  \num{ 61721}   &  \SI{56.98}{\percent}   & \num{1.117} \\
          \printdate{2018-08-01}   &  \num{ 43257}   &  \SI{61.98}{\percent}   & \num{1.158} \\
          \printdate{2018-08-21}   &  \num{ 90264}   &  \SI{69.31}{\percent}   & \num{3.194} \\
          \printdate{2018-08-22}   &  \num{ 85905}   &  \SI{68.05}{\percent}   & \num{2.065} \\
          \printdate{2018-08-23}   &  \num{103995}   &  \SI{71.50}{\percent}   & \num{3.455} \\
          \printdate{2018-08-24}   &  \num{119314}   &  \SI{71.95}{\percent}   & \num{4.680} \\
          \printdate{2018-08-26}   &  \num{107342}   &  \SI{73.23}{\percent}   & \num{1.927} \\
          \printdate{2018-08-30}   &  \num{108253}   &  \SI{68.97}{\percent}   & \num{3.807} \\
          \printdate{2018-09-03}   &  \num{ 72513}   &  \SI{63.37}{\percent}   & \num{1.078} \\
          \printdate{2018-09-05}   &  \num{ 32133}   &  \SI{75.60}{\percent}   & \num{1.397} \\
          \hline        
      \end{tabular}
  
    \caption{\oppositeCaption{\vw}}
    \label{tab:analysis-sentiments-vw-opposite}
\end{table}


\section{Comparison of Time Series}
\label{s:analysis-granger}
% granger causality

\subsection{\ford}
\label{ss:analysis-granger-ford}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/results-ford.tex}
    \caption{\resultsCaption{\ford}}
    \label{fig:analysis-results-ford}
\end{figure} 

\subsection{\gm}
\label{ss:analysis-granger-gm}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/results-gm.tex}
    \caption{\resultsCaption{\gm}}
    \label{fig:analysis-results-gm}
\end{figure} 

\subsection{\hyundai}
\label{ss:analysis-granger-hyundai}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/results-hyundai.tex}
    \caption{\resultsCaption{\hyundai}}
    \label{fig:analysis-results-hyundai}
\end{figure} 

\subsection{\toyota}
\label{ss:analysis-granger-toyota}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/results-toyota.tex}
    \caption{\resultsCaption{\toyota}}
    \label{fig:analysis-results-toyota}
\end{figure} 

\subsection{\vw}
\label{ss:analysis-granger-vw}

\begin{figure}[hbt]
    \centering
    \input{./images/graphs/results-vw.tex}
    \caption{\resultsCaption{\vw}}
    \label{fig:analysis-results-vw}
\end{figure} 
