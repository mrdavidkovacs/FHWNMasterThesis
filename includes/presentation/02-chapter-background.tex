%!TEX root = ../../presentation.tex

\section{Theoretical Background}

\begin{frame}
    \frametitle{Why Twitter?}

    \begin{outline}
        \1 Reliable in previous studies \citep{Barbosa2010}
            \2 Public opinion \citep{Oconnor2010a,Patodkar2016a}
            \2 Stock market prediction \citep{Bollen2011a,Mittal2012a,Nguyen2015a,Pagolu2016a,Zhang2011a}

        \1 Short messages of up to 280 characters \citep{Rosen2017}
        \1 One topic is assumed due limited characters \citep{Pagolu2016a,Patodkar2016a}
    \end{outline}
\end{frame}
  
\note[itemize]{
    \item Page 11
}

\begin{frame}
    \frametitle{Sentiment Detection Algorithms}

    \begin{outline}
        \1 \tb{}
        \1 \nb{}
        \1 \me{}
        \1 \svm{}
    \end{outline}
\end{frame}

\note[itemize] {
    \item Pages 8ff
    \item TextBlob: No training; Baseline
    \item Naive Bayes (probabilistic algorithm) \citep{Pang2002}

    \item Maximum Entropy (probabilistic algorithm) \citep{Pang2002}

        $\lambda_{i,c}$ are weight parameters which are optimized during training.

    \item Support Vector Machine (large margin classifier) \citep{Pang2002}

    \item Accuracy: $\frac{TP+TN}{TP+TN+FN+FP}$
    \item F1-Measure: $\frac{2*precision*recall}{precision+recall}$
    \item Precision: $\frac{TP}{TP+FP}$
    \item Recall: $\frac{TP}{TP+TN}$
}